{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3112fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec01f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import requests\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e7835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b747e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056354d0",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 \n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328b9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51e8c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field\n",
    "\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "search_field_designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bc4d3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ce7bad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae921cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b52eb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping job title from the given page\n",
    "\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    \n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping the job-location\n",
    "\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    \n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "#scraping the company name\n",
    "\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    \n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "##scraping the experience_required \n",
    "\n",
    "experience_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in experience_tags[0:10]:\n",
    "    \n",
    "    exp = i.text\n",
    "    experience_required.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1c7552d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "00c70af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>EXP_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contractual Hiring For Top MNC || Business Dat...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>TeamLease</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HCL hiring For Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customer Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "      <td>Global Indian School Education Services</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Decision Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Jana Small Finance Bank</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Programmer / Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "      <td>Frost &amp; Sullivan</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Immediate opening For Data Analyst @ Bangalore</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>TeamLease</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Domlur)</td>\n",
       "      <td>KrazyBee</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  Contractual Hiring For Top MNC || Business Dat...   \n",
       "1                        HCL hiring For Data Analyst   \n",
       "2                              Customer Data Analyst   \n",
       "3                                   Sr. Data Analyst   \n",
       "4                    Data Analyst - Decision Science   \n",
       "5                          Programmer / Data Analyst   \n",
       "6     Immediate opening For Data Analyst @ Bangalore   \n",
       "7                             Associate Data Analyst   \n",
       "8                             Associate Data Analyst   \n",
       "9                                Senior Data Analyst   \n",
       "\n",
       "                                        job_location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                 Bangalore/Bengaluru, Pune, Chennai   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                          Bangalore/Bengaluru, Pune   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                        Bangalore/Bengaluru(Domlur)   \n",
       "\n",
       "                              company_name EXP_required  \n",
       "0                                TeamLease      5-8 Yrs  \n",
       "1                         HCL Technologies      3-8 Yrs  \n",
       "2                                   Oracle      1-3 Yrs  \n",
       "3  Global Indian School Education Services     6-11 Yrs  \n",
       "4                  Jana Small Finance Bank      3-8 Yrs  \n",
       "5                         Frost & Sullivan      3-7 Yrs  \n",
       "6                                TeamLease      4-6 Yrs  \n",
       "7                                    Optum      2-7 Yrs  \n",
       "8                                    Optum      1-4 Yrs  \n",
       "9                                 KrazyBee      3-5 Yrs  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'job_title':job_title, 'job_location':job_location, 'company_name':company_name, 'EXP_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfc03a",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f9d9d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4233d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "be586ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter “Data science” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field\n",
    "\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "search_field_designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "07b433ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c57616ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7fc7c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b057e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping job title from the given page\n",
    "\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    \n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping the job-location\n",
    "\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    \n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "#scraping the company name\n",
    "\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    \n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c84f59df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Opportunity on Data Science_ Python with T...</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>CitiusTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist/AIML Engineer</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>upGrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACN - Applied Intelligence - C4DI - Sustainabi...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead ML Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tcs Hiring For Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai, Mumbai (All Areas)</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "      <td>SMARTPADDLE TECHNOLOGY PVT. LTD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - A.P. Maersk</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>Maersk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  Job Opportunity on Data Science_ Python with T...   \n",
       "1                   Assistant Manager - Data Science   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3  Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "4                       Data Scientist/AIML Engineer   \n",
       "5  ACN - Applied Intelligence - C4DI - Sustainabi...   \n",
       "6                                  Lead ML Scientist   \n",
       "7                      Tcs Hiring For Data Scientist   \n",
       "8                                Data Scientist - II   \n",
       "9                       Data Scientist - A.P. Maersk   \n",
       "\n",
       "                                        job_location  \\\n",
       "0  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...   \n",
       "1                  Bangalore/Bengaluru, Mumbai, Pune   \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "3  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "4  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                        Bangalore/Bengaluru, Mumbai   \n",
       "7   Bangalore/Bengaluru, Chennai, Mumbai (All Areas)   \n",
       "8     Bangalore/Bengaluru, India, Mumbai (All Areas)   \n",
       "9            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "\n",
       "                                  company_name  \n",
       "0                                Tech Mahindra  \n",
       "1                                   CitiusTech  \n",
       "2                                    Accenture  \n",
       "3  NTT DATA Business Solutions Private Limited  \n",
       "4                                       upGrad  \n",
       "5                                    Accenture  \n",
       "6                            Fractal Analytics  \n",
       "7              TATA CONSULTANCY SERVICES (TCS)  \n",
       "8             SMARTPADDLE TECHNOLOGY PVT. LTD.  \n",
       "9                                       Maersk  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'job_title':job_title, 'job_location':job_location, 'company_name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60d8a5",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "ASSIGNMENT 2\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e87f367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter “Data science” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field\n",
    "\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "search_field_designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "177f11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Delhi/NCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "461a9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ed4b2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "133e44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    \n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping the job-location\n",
    "\n",
    "location_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    \n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "    \n",
    "#scraping the company name\n",
    "\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    \n",
    "    company = i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    " ##scraping the experience_required \n",
    "\n",
    "experience_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in experience_tags[0:10]:\n",
    "    \n",
    "    exp = i.text\n",
    "    experience_required.append(exp)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "43086489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>EXP_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Opportunity on Data Science_ Python with T...</td>\n",
       "      <td>Delhi / NCR, Kolkata, Hyderabad/Secunderabad, ...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Pune, Gurgaon/G...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Nagpur, Bangalore/Bengaluru</td>\n",
       "      <td>GlobalLogic</td>\n",
       "      <td>8-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVAP Agile Information Systems - Data Scientis...</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>AVAP Agile</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist Lead</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Cloudjune</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, New Delhi, Pune, Gurgaon/Gurugram...</td>\n",
       "      <td>ZS Associates</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>Paytm</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru\\n(WFH du...</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Science - Senior Software Engineer</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>Paytm</td>\n",
       "      <td>8-15 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  Job Opportunity on Data Science_ Python with T...   \n",
       "1                   Analystics & Modeling Specialist   \n",
       "2  Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "3                                     Data Scientist   \n",
       "4  AVAP Agile Information Systems - Data Scientis...   \n",
       "5                                Data Scientist Lead   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8    Forecasting Analyst/ Data Scientist (US Client)   \n",
       "9            Data Science - Senior Software Engineer   \n",
       "\n",
       "                                        job_location  \\\n",
       "0  Delhi / NCR, Kolkata, Hyderabad/Secunderabad, ...   \n",
       "1  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "2  Noida, Hyderabad/Secunderabad, Pune, Gurgaon/G...   \n",
       "3                 Noida, Nagpur, Bangalore/Bengaluru   \n",
       "4  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "5  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "6  Delhi / NCR, New Delhi, Pune, Gurgaon/Gurugram...   \n",
       "7     Noida, Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "8  Gurgaon/Gurugram, Bangalore/Bengaluru\\n(WFH du...   \n",
       "9                         Noida, Bangalore/Bengaluru   \n",
       "\n",
       "                                  company_name EXP_required  \n",
       "0                                Tech Mahindra      4-9 Yrs  \n",
       "1                                    Accenture      6-8 Yrs  \n",
       "2  NTT DATA Business Solutions Private Limited      4-9 Yrs  \n",
       "3                                  GlobalLogic     8-10 Yrs  \n",
       "4                                   AVAP Agile      3-8 Yrs  \n",
       "5                                    Cloudjune     6-10 Yrs  \n",
       "6                                ZS Associates      5-8 Yrs  \n",
       "7                                        Paytm      1-2 Yrs  \n",
       "8                                   Concentrix      3-8 Yrs  \n",
       "9                                        Paytm     8-15 Yrs  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'job_title':job_title, 'job_location':job_location, 'company_name':company_name, 'EXP_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999ccba",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c9252034",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bce3397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/\" \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b90c6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "search_bar.send_keys('sunglasses')\n",
    "\n",
    "\n",
    "#locating the button and clicking it toh search for sunglasses\n",
    "button=driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fa5e5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scrapping the required details\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    brands=driver.find_elements(By.CLASS_NAME,'_2WkVRV')\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    desc=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\" or @class=\"IRpwTa _2-ICcC\"]')\n",
    "    for i in desc:\n",
    "        description.append(i.text)\n",
    "    prices=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")\n",
    "    \n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dd4feec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Round Sunglasses (51)</td>\n",
       "      <td>₹1,128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (55)</td>\n",
       "      <td>₹189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Retro Squar...</td>\n",
       "      <td>₹246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EYELLUSION</td>\n",
       "      <td>Riding Glasses, Riding Glasses, UV Protection ...</td>\n",
       "      <td>₹189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>CRYSTAL CART</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (52)</td>\n",
       "      <td>₹639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹1,099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection, Riding Glasses Rectangular, Ret...</td>\n",
       "      <td>₹375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Polarized Retro Square Sunglasses (47)</td>\n",
       "      <td>₹384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand                                        Description   Price\n",
       "0     VINCENT CHASE     Polarized, UV Protection Round Sunglasses (51)  ₹1,128\n",
       "1     VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...    ₹699\n",
       "2    kingsunglasses          UV Protection Rectangular Sunglasses (55)    ₹189\n",
       "3            SUNBEE  UV Protection, Polarized, Mirrored Retro Squar...    ₹246\n",
       "4        EYELLUSION  Riding Glasses, Riding Glasses, UV Protection ...    ₹189\n",
       "..              ...                                                ...     ...\n",
       "115    CRYSTAL CART  UV Protection, Gradient Round Sunglasses (Free...    ₹377\n",
       "116        Fastrack             UV Protection Wayfarer Sunglasses (52)    ₹639\n",
       "117   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...  ₹1,099\n",
       "118       Elligator  UV Protection, Riding Glasses Rectangular, Ret...    ₹375\n",
       "119          PIRASO             Polarized Retro Square Sunglasses (47)    ₹384\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand,\n",
    "                'Description':description,\n",
    "                'Price':price})\n",
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e2157",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field . \n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "00ced37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b9c54016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Stars</th>\n",
       "      <th>Short Review</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>It is better to buy iPhone 11 over iPhone 12 i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>It was amazing experience for me. Honestly i a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>I bought iPhone 11 On March 2021, And I am Wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Just go for it.\\nThis phone is really amazing....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.\\nAnd I truly don...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Stars         Short Review  \\\n",
       "0                5       Simply awesome   \n",
       "1                5     Perfect product!   \n",
       "2                5  Best in the market!   \n",
       "3                5   Highly recommended   \n",
       "4                5    Worth every penny   \n",
       "..             ...                  ...   \n",
       "95               5    Worth every penny   \n",
       "96               5            Excellent   \n",
       "97               5             Terrific   \n",
       "98               5            Excellent   \n",
       "99               5               Super!   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   What a camera .....just awesome ..you can feel...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  It is better to buy iPhone 11 over iPhone 12 i...  \n",
       "96  It was amazing experience for me. Honestly i a...  \n",
       "97  I bought iPhone 11 On March 2021, And I am Wri...  \n",
       "98  Just go for it.\\nThis phone is really amazing....  \n",
       "99  This is my first ever iPhone.\\nAnd I truly don...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating empty list\n",
    "urls=[]\n",
    "short_review=[]\n",
    "complete_review=[]\n",
    "stars=[]\n",
    "time.sleep(2)\n",
    "\n",
    "#scraping 10 pages url\n",
    "url_1 = driver.find_elements(By.XPATH,\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "for i in url_1:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "url_2 = driver.find_elements(By.XPATH,\"//a[@class='ge-49M']\")\n",
    "for i in url_2:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    #for scrapping the number of stars\n",
    "    for j in driver.find_elements(By.XPATH,\"//div[@class='col _2wzgFH K0kLPL']/div[1]/div[1]\"):\n",
    "        stars.append(j.text)\n",
    "    #for scrapping the short review\n",
    "    for k in driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\"):\n",
    "        short_review.append(k.text)\n",
    "    #for scrapping the complete review\n",
    "    for l in driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']/div/div\"):\n",
    "        complete_review.append(l.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Combining all the lists into a single dataframe\n",
    "df=pd.DataFrame({'Number of Stars': stars,\n",
    "                'Short Review': short_review,\n",
    "               'Full Review': complete_review})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba49de",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the \n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "ASSIGNMENT 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "447b93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "search_bar.send_keys('sneakers')\n",
    "\n",
    "\n",
    "#locating the button and clicking it to search for sneakers\n",
    "button=driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b6ee4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    brands=driver.find_elements(By.CLASS_NAME,'_2WkVRV')\n",
    "    for i in brands:\n",
    "        brand.append(i.text)\n",
    "    prices=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    desc=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\" or @class=\"IRpwTa _2-ICcC\"]')\n",
    "    for i in desc:\n",
    "        description.append(i.text)\n",
    "        \n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cedab1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Casual Sneakers White Shoes For Girls And Snea...</td>\n",
       "      <td>₹499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>₹352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labbin</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOTSTYLE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Lightweight Pack Of 1 Trendy Sneakers Sneakers...</td>\n",
       "      <td>₹179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>asian</td>\n",
       "      <td>Skypy-31 Walking Shoes,Training Shoes,Sneakers...</td>\n",
       "      <td>₹419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Aragats</td>\n",
       "      <td>Casual Sneakers For Men</td>\n",
       "      <td>₹521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>₹322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Latest Collection Black-349 Trendy &amp; Stylish C...</td>\n",
       "      <td>₹249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Sparx</td>\n",
       "      <td>SM-620 Sneakers For Men</td>\n",
       "      <td>₹909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                        Description  \\\n",
       "0                Layasa  Casual Sneakers White Shoes For Girls And Snea...   \n",
       "1                BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men   \n",
       "2                Labbin                                   Sneakers For Men   \n",
       "3              HOTSTYLE                                   Sneakers For Men   \n",
       "4                BRUTON  Lightweight Pack Of 1 Trendy Sneakers Sneakers...   \n",
       "..                  ...                                                ...   \n",
       "95                asian  Skypy-31 Walking Shoes,Training Shoes,Sneakers...   \n",
       "96              Aragats                            Casual Sneakers For Men   \n",
       "97               BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men   \n",
       "98  World Wear Footwear  Latest Collection Black-349 Trendy & Stylish C...   \n",
       "99                Sparx                            SM-620 Sneakers For Men   \n",
       "\n",
       "   Price  \n",
       "0   ₹499  \n",
       "1   ₹352  \n",
       "2   ₹329  \n",
       "3   ₹292  \n",
       "4   ₹179  \n",
       "..   ...  \n",
       "95  ₹419  \n",
       "96  ₹521  \n",
       "97  ₹322  \n",
       "98  ₹249  \n",
       "99  ₹909  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],'Description':description[:100],'Price':price[:100]})\n",
    "df\n",
    "                \n",
    "              \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49628d42",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe \n",
    "description, price of the shoe as shown in the below image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "62289598",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e16a20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on price filter\n",
    "button = driver.find_element(By.XPATH,\"//ul[@class='price-list']/li[2]\")\n",
    "button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bc5c840d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Short-description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Winflo 8 Running Shoes</td>\n",
       "      <td>Rs. 7880Rs. 8295(5% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Niteball II Sneakers</td>\n",
       "      <td>Rs. 9349Rs. 10999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men ZOOM WINFLO8 Running Shoes</td>\n",
       "      <td>Rs. 7880Rs. 8295(5% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Men Lightweight Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Men Solid Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men Slip-On Sneakers</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Unisex X90000L3 U Running Shoe</td>\n",
       "      <td>Rs. 10495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Superstar Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Running Shoes</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand               Short-description  \\\n",
       "0               Nike      Men Winflo 8 Running Shoes   \n",
       "1   ADIDAS Originals        Men Niteball II Sneakers   \n",
       "2               Nike  Men ZOOM WINFLO8 Running Shoes   \n",
       "3     Tommy Hilfiger        Men Lightweight Sneakers   \n",
       "4     Tommy Hilfiger              Men Solid Sneakers   \n",
       "..               ...                             ...   \n",
       "95            ADIDAS            Men Slip-On Sneakers   \n",
       "96              Nike  Unisex X90000L3 U Running Shoe   \n",
       "97    Tommy Hilfiger            Men Leather Sneakers   \n",
       "98  ADIDAS Originals          Men Superstar Sneakers   \n",
       "99  ADIDAS Originals               Men Running Shoes   \n",
       "\n",
       "                         Price  \n",
       "0     Rs. 7880Rs. 8295(5% OFF)  \n",
       "1   Rs. 9349Rs. 10999(15% OFF)  \n",
       "2     Rs. 7880Rs. 8295(5% OFF)  \n",
       "3                     Rs. 7999  \n",
       "4                     Rs. 9999  \n",
       "..                         ...  \n",
       "95                    Rs. 8999  \n",
       "96                   Rs. 10495  \n",
       "97                    Rs. 8299  \n",
       "98                    Rs. 9999  \n",
       "99                    Rs. 9999  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating empty lists\n",
    "shoe_names=[]\n",
    "shoe_desc=[]\n",
    "short_desc=[]\n",
    "price=[]\n",
    "page_urls = []\n",
    "\n",
    "\n",
    "# scrape next pages urls\n",
    "nxt_page = driver.find_elements(By.XPATH,\"//ul[@class='pagination-container']/li/a\")\n",
    "for i in nxt_page:\n",
    "    page_urls.append(i.get_attribute('href'))\n",
    "    \n",
    "\n",
    "for url in page_urls[:3]:\n",
    "    driver.get(url)\n",
    "    Names=driver.find_elements(By.XPATH,\"//div[@class='product-productMetaInfo']/h3\")  \n",
    "    for i in Names:\n",
    "        shoe_names.append(i.text)\n",
    "    \n",
    "    desc=driver.find_elements(By.XPATH,\"//div[@class='product-productMetaInfo']/h4\") \n",
    "    for i in desc:\n",
    "        shoe_desc.append(i.text)\n",
    "    \n",
    "    for j in range(0,len(shoe_desc),2):\n",
    "        short_desc.append(shoe_desc[j])\n",
    "    \n",
    "    rs=driver.find_elements(By.XPATH,\"//div[@class='product-price']\")  \n",
    "    for i in rs:\n",
    "        price.append(i.text)    \n",
    "        \n",
    "df=pd.DataFrame({'Brand': shoe_names[:100],'Short-description': short_desc[:100],'Price': price[:100]})\n",
    "df  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281be9a0",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "   After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fd9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60315f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "laptop.send_keys('laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918f7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding search\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c26797",
   "metadata": {},
   "outputs": [],
   "source": [
    "intel = driver.find_element(By.XPATH,'//li[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]')\n",
    "intel.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c1baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "title = []\n",
    "laptop_rating = []\n",
    "laptop_price = []\n",
    "\n",
    "#title scraping\n",
    "laptop_title = driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in laptop_title[0:10]:\n",
    "    lap_title = i.text\n",
    "    title.append(lap_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc65ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping price\n",
    "lap_price = driver.find_elements(By.XPATH, '//span[@class=\"a-price-whole\"]')\n",
    "for i in lap_price[0:10]:\n",
    "    laptop_price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20442bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(title),len(laptop_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830db748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping ratings\n",
    "url = driver.find_elements(By.XPATH,\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")\n",
    "url1 = []\n",
    "\n",
    "\n",
    "for i in url[:10]:\n",
    "    url1.append(i.get_attribute('href'))\n",
    "for url2 in url1:\n",
    "    driver.get(url2)\n",
    "    try:                  \n",
    "        rating = driver.find_element(By.XPATH, \"//span[@class='a-size-base a-nowrap']//span\")\n",
    "        laptop_rating.append(rating.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        laptop_rating.append(\"NO rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d814af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4be9e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_4be9e_level0_col0\" class=\"col_heading level0 col0\" >Title</th>\n",
       "      <th id=\"T_4be9e_level0_col1\" class=\"col_heading level0 col1\" >Ratings</th>\n",
       "      <th id=\"T_4be9e_level0_col2\" class=\"col_heading level0 col2\" >Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row0_col0\" class=\"data row0 col0\" >Acer Aspire 3 Laptop AMD 3020e Dual-Core Processor/Win11 Home/4 GB/256GB SSD/1.9kgs 35.56 cm (14-inches) HD Display, A314-22, Windows 11 Home</td>\n",
       "      <td id=\"T_4be9e_row0_col1\" class=\"data row0 col1\" >3.3 out of 5</td>\n",
       "      <td id=\"T_4be9e_row0_col2\" class=\"data row0 col2\" >24,489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row1_col0\" class=\"data row1 col0\" >Acer Extensa 15 Lightweight Laptop 11th Gen Intel Core i3 Processor with 15.6\" (39.6 cms) Full HD Display- (4 GB RAM/256GB SSD/Windows 11 Home/Intel UHD Graphics /1.7Kg/Black) EX215-54</td>\n",
       "      <td id=\"T_4be9e_row1_col1\" class=\"data row1 col1\" >4 out of 5</td>\n",
       "      <td id=\"T_4be9e_row1_col2\" class=\"data row1 col2\" >29,790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row2_col0\" class=\"data row2 col0\" >Lenovo IdeaPad Slim 1 Intel Celeron N4020 11.6'' (29.46cm) HD Thin & Light Laptop (4GB/256GB SSD/Windows 11/Office 2021/3months Game Pass/Platinum Grey/1.2Kg), 81VT009UIN</td>\n",
       "      <td id=\"T_4be9e_row2_col1\" class=\"data row2 col1\" >3.9 out of 5</td>\n",
       "      <td id=\"T_4be9e_row2_col2\" class=\"data row2 col2\" >21,513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row3_col0\" class=\"data row3 col0\" >(Renewed) HP Stream 11 G4 11.6in LCD Laptop- Intel Celeron N3350 Dual-core (2 Core) 1.10 GHz - 4 GB DDR3L SDRAM - 64 GB Flash Memory Windows 10</td>\n",
       "      <td id=\"T_4be9e_row3_col1\" class=\"data row3 col1\" >NO rating</td>\n",
       "      <td id=\"T_4be9e_row3_col2\" class=\"data row3 col2\" >14,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row4_col0\" class=\"data row4 col0\" >ASUS VivoBook 15 (2021), 15.6-inch (39.62 cm) HD, Dual Core Intel Celeron N4020, Thin and Light Laptop (4GB RAM/256GB SSD/Integrated Graphics/Windows 11 Home/Transparent Silver/1.8 Kg), X515MA-BR011W</td>\n",
       "      <td id=\"T_4be9e_row4_col1\" class=\"data row4 col1\" >4.1 out of 5</td>\n",
       "      <td id=\"T_4be9e_row4_col2\" class=\"data row4 col2\" >25,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row5_col0\" class=\"data row5 col0\" >MSI Modern 14, Intel i5-1155G7, 14\"(35cm) FHD IPS-Level 60Hz Panel Laptop (8GB/512GB NVMe SSD/Windows 10 Home/Intel UHD Graphics/Carbon Grey/1.3Kg), B11MOU-861IN</td>\n",
       "      <td id=\"T_4be9e_row5_col1\" class=\"data row5 col1\" >4.2 out of 5</td>\n",
       "      <td id=\"T_4be9e_row5_col2\" class=\"data row5 col2\" >45,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row6_col0\" class=\"data row6 col0\" >Lenovo IdeaPad Slim 3 Intel Celeron N4020 15.6\" (39.62cm) HD Thin & Light Laptop (8GB/256GB SSD/Windows 11/Office 2021/2Yr Warranty/3months Game Pass/Platinum Grey/1.7Kg), 81WQ00MQIN</td>\n",
       "      <td id=\"T_4be9e_row6_col1\" class=\"data row6 col1\" >3.6 out of 5</td>\n",
       "      <td id=\"T_4be9e_row6_col2\" class=\"data row6 col2\" >51,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row7_col0\" class=\"data row7 col0\" >Redmi Book 15 Intel Core I3 11Th Gen/8 Gb/256 Gb Ssd/Windows 11 Home/15.6 Inches (39.62 Cms) Fhd Anti Glare/Ms Office/Charcoal Gray/1.8 Kg Thin and Light Laptop</td>\n",
       "      <td id=\"T_4be9e_row7_col1\" class=\"data row7 col1\" >4.3 out of 5</td>\n",
       "      <td id=\"T_4be9e_row7_col2\" class=\"data row7 col2\" >42,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row8_col0\" class=\"data row8 col0\" >HP 14s, 5th Gen AMD Ryzen 3- 8GB RAM/512GB SSD 14 inches(35cm) Laptop, FHD IPS Micro-Edge Display/ Backlit Keyboard/Alexa/Windows 11/Fast Charge/Radeon Graphics/1.46Kg/Natural Silver) -14s-fq1089AU</td>\n",
       "      <td id=\"T_4be9e_row8_col1\" class=\"data row8 col1\" >3.7 out of 5</td>\n",
       "      <td id=\"T_4be9e_row8_col2\" class=\"data row8 col2\" >79,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4be9e_row9_col0\" class=\"data row9 col0\" >(Renewed) Dell Latitude E5470 Intel Core i5 6th Gen.6200u Processor 14.1 Inches HD Screen Notebook Computer (8 GB Ram & 256 GB SSD, Windows 10 Pro, 1.71Kg)</td>\n",
       "      <td id=\"T_4be9e_row9_col1\" class=\"data row9 col1\" >3.9 out of 5</td>\n",
       "      <td id=\"T_4be9e_row9_col2\" class=\"data row9 col2\" >29,374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2253a22dd00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating dataframe\n",
    "laptop_df = pd.DataFrame({'Title':title, 'Ratings': laptop_rating, 'Price':laptop_price})\n",
    "laptop_df = laptop_df.style.hide_index()\n",
    "laptop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c19a16",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida \n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. \n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "ASSIGNMENT 2\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d07fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.ambitionbox.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122a7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'/html/body/div/div/div/div[1]/header/nav/ul/li[5]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad0a4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = driver.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input')\n",
    "data.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b9426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f0490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#location\n",
    "driver.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/p').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f840c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "location.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377a6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63450da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.1', '4.3', '3.8', '4.0', '4.0', '4.0', '4.3', '4.3', '3.8', '3.8']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating empty list\n",
    "Company = []\n",
    "no_day = []\n",
    "c_rating = []\n",
    "\n",
    "#Scraping first 10 company names.\n",
    "c_name = driver.find_elements(By.XPATH, '//p[@class=\"company body-medium\"]')\n",
    "for i in c_name[0:10]:\n",
    "    Company.append(i.text)\n",
    "    \n",
    "#Scraping No of days ago when job posted\n",
    "days = driver.find_elements(By.XPATH, \"//div[@class='other-info']/span[1]\")\n",
    "for i in days[0:20]:\n",
    "    nos = i.text\n",
    "    no_day.append(nos)\n",
    "    \n",
    "#Scraping rating of the company\n",
    "com_rat = driver.find_elements(By.XPATH, '//span[@class=\"body-small\"]')\n",
    "for i in com_rat[0:10]:\n",
    "    c_rating.append(i.text)\n",
    "    \n",
    "c_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d60306b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_858bb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_858bb_level0_col0\" class=\"col_heading level0 col0\" >Company Name</th>\n",
       "      <th id=\"T_858bb_level0_col1\" class=\"col_heading level0 col1\" >No Of Days Ago Job Posted</th>\n",
       "      <th id=\"T_858bb_level0_col2\" class=\"col_heading level0 col2\" >Company Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row0_col0\" class=\"data row0 col0\" >Optum Global Solutions (India) Private Limited</td>\n",
       "      <td id=\"T_858bb_row0_col1\" class=\"data row0 col1\" >6d ago</td>\n",
       "      <td id=\"T_858bb_row0_col2\" class=\"data row0 col2\" >4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row1_col0\" class=\"data row1 col0\" >BARCLAYS GLOBAL SERVICE CENTRE PRIVATE LIMITED</td>\n",
       "      <td id=\"T_858bb_row1_col1\" class=\"data row1 col1\" >4d ago</td>\n",
       "      <td id=\"T_858bb_row1_col2\" class=\"data row1 col2\" >4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row2_col0\" class=\"data row2 col0\" >EY GDS</td>\n",
       "      <td id=\"T_858bb_row2_col1\" class=\"data row2 col1\" >6d ago</td>\n",
       "      <td id=\"T_858bb_row2_col2\" class=\"data row2 col2\" >3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row3_col0\" class=\"data row3 col0\" >GLOBALLOGIC INDIA PRIVATE LIMITED</td>\n",
       "      <td id=\"T_858bb_row3_col1\" class=\"data row3 col1\" >4d ago</td>\n",
       "      <td id=\"T_858bb_row3_col2\" class=\"data row3 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row4_col0\" class=\"data row4 col0\" >GENPACT India Private Limited</td>\n",
       "      <td id=\"T_858bb_row4_col1\" class=\"data row4 col1\" >26d ago</td>\n",
       "      <td id=\"T_858bb_row4_col2\" class=\"data row4 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row5_col0\" class=\"data row5 col0\" >Genpact</td>\n",
       "      <td id=\"T_858bb_row5_col1\" class=\"data row5 col1\" >28d ago</td>\n",
       "      <td id=\"T_858bb_row5_col2\" class=\"data row5 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row6_col0\" class=\"data row6 col0\" >Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td id=\"T_858bb_row6_col1\" class=\"data row6 col1\" >1mon ago</td>\n",
       "      <td id=\"T_858bb_row6_col2\" class=\"data row6 col2\" >4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row7_col0\" class=\"data row7 col0\" >Dew Solutions Pvt. Ltd.</td>\n",
       "      <td id=\"T_858bb_row7_col1\" class=\"data row7 col1\" >11d ago</td>\n",
       "      <td id=\"T_858bb_row7_col2\" class=\"data row7 col2\" >4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row8_col0\" class=\"data row8 col0\" >One97 Communications Limited</td>\n",
       "      <td id=\"T_858bb_row8_col1\" class=\"data row8 col1\" >19d ago</td>\n",
       "      <td id=\"T_858bb_row8_col2\" class=\"data row8 col2\" >3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_858bb_row9_col0\" class=\"data row9 col0\" >EY</td>\n",
       "      <td id=\"T_858bb_row9_col1\" class=\"data row9 col1\" >1mon ago</td>\n",
       "      <td id=\"T_858bb_row9_col2\" class=\"data row9 col2\" >3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2253c09e4f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Daframe to showcase scrapped data\n",
    "job = pd.DataFrame({'Company Name':Company, 'No Of Days Ago Job Posted':no_day, 'Company Rating':c_rating})\n",
    "job = job.style.hide_index()\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53212269",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "72fe075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening Ambitionbox page on automated webdriver\n",
    "driver.get('https://www.ambitionbox.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e72b2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on salary\n",
    "time.sleep(3)\n",
    "driver.find_element(By.XPATH, '/html/body/div/div/div/div[1]/header/nav/ul/li[3]/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b69abfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on Browse salaries \n",
    "driver.find_element(By.XPATH, '/html/body/div/div/div/div[1]/header/nav/ul/li[3]/div/ul/li[1]/div/div[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bec395fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Data Scientist\n",
    "time.sleep(2)\n",
    "ds = driver.find_element(By.XPATH, '/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input')\n",
    "ds.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "42176617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on Data Scientist\n",
    "data = driver.find_elements(By.CLASS_NAME, 'tt_text')\n",
    "data[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d64376f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_19bc3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_19bc3_level0_col0\" class=\"col_heading level0 col0\" >Company Name</th>\n",
       "      <th id=\"T_19bc3_level0_col1\" class=\"col_heading level0 col1\" >Total No Of Records</th>\n",
       "      <th id=\"T_19bc3_level0_col2\" class=\"col_heading level0 col2\" >Average Salary</th>\n",
       "      <th id=\"T_19bc3_level0_col3\" class=\"col_heading level0 col3\" >Minimun Salary</th>\n",
       "      <th id=\"T_19bc3_level0_col4\" class=\"col_heading level0 col4\" >Maximum Salary</th>\n",
       "      <th id=\"T_19bc3_level0_col5\" class=\"col_heading level0 col5\" >Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row0_col0\" class=\"data row0 col0\" >Walmart</td>\n",
       "      <td id=\"T_19bc3_row0_col1\" class=\"data row0 col1\" >based on 23 salaries</td>\n",
       "      <td id=\"T_19bc3_row0_col2\" class=\"data row0 col2\" > 32.3L</td>\n",
       "      <td id=\"T_19bc3_row0_col3\" class=\"data row0 col3\" >25.0L</td>\n",
       "      <td id=\"T_19bc3_row0_col4\" class=\"data row0 col4\" >45.0L</td>\n",
       "      <td id=\"T_19bc3_row0_col5\" class=\"data row0 col5\" >3-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row1_col0\" class=\"data row1 col0\" >Ab Inbev</td>\n",
       "      <td id=\"T_19bc3_row1_col1\" class=\"data row1 col1\" >based on 57 salaries</td>\n",
       "      <td id=\"T_19bc3_row1_col2\" class=\"data row1 col2\" > 19.9L</td>\n",
       "      <td id=\"T_19bc3_row1_col3\" class=\"data row1 col3\" >15.0L</td>\n",
       "      <td id=\"T_19bc3_row1_col4\" class=\"data row1 col4\" >26.0L</td>\n",
       "      <td id=\"T_19bc3_row1_col5\" class=\"data row1 col5\" >2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row2_col0\" class=\"data row2 col0\" >Optum</td>\n",
       "      <td id=\"T_19bc3_row2_col1\" class=\"data row2 col1\" >based on 49 salaries</td>\n",
       "      <td id=\"T_19bc3_row2_col2\" class=\"data row2 col2\" > 16.4L</td>\n",
       "      <td id=\"T_19bc3_row2_col3\" class=\"data row2 col3\" >11.0L</td>\n",
       "      <td id=\"T_19bc3_row2_col4\" class=\"data row2 col4\" >22.6L</td>\n",
       "      <td id=\"T_19bc3_row2_col5\" class=\"data row2 col5\" >2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row3_col0\" class=\"data row3 col0\" >ZS</td>\n",
       "      <td id=\"T_19bc3_row3_col1\" class=\"data row3 col1\" >based on 34 salaries</td>\n",
       "      <td id=\"T_19bc3_row3_col2\" class=\"data row3 col2\" > 15.8L</td>\n",
       "      <td id=\"T_19bc3_row3_col3\" class=\"data row3 col3\" >11.0L</td>\n",
       "      <td id=\"T_19bc3_row3_col4\" class=\"data row3 col4\" >22.0L</td>\n",
       "      <td id=\"T_19bc3_row3_col5\" class=\"data row3 col5\" >1-2 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row4_col0\" class=\"data row4 col0\" >Fractal Analytics</td>\n",
       "      <td id=\"T_19bc3_row4_col1\" class=\"data row4 col1\" >based on 115 salaries</td>\n",
       "      <td id=\"T_19bc3_row4_col2\" class=\"data row4 col2\" > 15.4L</td>\n",
       "      <td id=\"T_19bc3_row4_col3\" class=\"data row4 col3\" >9.0L</td>\n",
       "      <td id=\"T_19bc3_row4_col4\" class=\"data row4 col4\" >23.0L</td>\n",
       "      <td id=\"T_19bc3_row4_col5\" class=\"data row4 col5\" >2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row5_col0\" class=\"data row5 col0\" >Tiger Analytics</td>\n",
       "      <td id=\"T_19bc3_row5_col1\" class=\"data row5 col1\" >based on 68 salaries</td>\n",
       "      <td id=\"T_19bc3_row5_col2\" class=\"data row5 col2\" > 14.7L</td>\n",
       "      <td id=\"T_19bc3_row5_col3\" class=\"data row5 col3\" >9.0L</td>\n",
       "      <td id=\"T_19bc3_row5_col4\" class=\"data row5 col4\" >20.0L</td>\n",
       "      <td id=\"T_19bc3_row5_col5\" class=\"data row5 col5\" >2-4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row6_col0\" class=\"data row6 col0\" >Sigmoid Analytics</td>\n",
       "      <td id=\"T_19bc3_row6_col1\" class=\"data row6 col1\" >based on 10 salaries</td>\n",
       "      <td id=\"T_19bc3_row6_col2\" class=\"data row6 col2\" > 14.7L</td>\n",
       "      <td id=\"T_19bc3_row6_col3\" class=\"data row6 col3\" >12.7L</td>\n",
       "      <td id=\"T_19bc3_row6_col4\" class=\"data row6 col4\" >19.7L</td>\n",
       "      <td id=\"T_19bc3_row6_col5\" class=\"data row6 col5\" >1 yr experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row7_col0\" class=\"data row7 col0\" >Legato Health Technologies</td>\n",
       "      <td id=\"T_19bc3_row7_col1\" class=\"data row7 col1\" >based on 11 salaries</td>\n",
       "      <td id=\"T_19bc3_row7_col2\" class=\"data row7 col2\" > 14.5L</td>\n",
       "      <td id=\"T_19bc3_row7_col3\" class=\"data row7 col3\" >11.0L</td>\n",
       "      <td id=\"T_19bc3_row7_col4\" class=\"data row7 col4\" >20.0L</td>\n",
       "      <td id=\"T_19bc3_row7_col5\" class=\"data row7 col5\" >4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row8_col0\" class=\"data row8 col0\" >HSBC</td>\n",
       "      <td id=\"T_19bc3_row8_col1\" class=\"data row8 col1\" >based on 10 salaries</td>\n",
       "      <td id=\"T_19bc3_row8_col2\" class=\"data row8 col2\" > 14.0L</td>\n",
       "      <td id=\"T_19bc3_row8_col3\" class=\"data row8 col3\" >12.0L</td>\n",
       "      <td id=\"T_19bc3_row8_col4\" class=\"data row8 col4\" >18.0L</td>\n",
       "      <td id=\"T_19bc3_row8_col5\" class=\"data row8 col5\" >4 yrs experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_19bc3_row9_col0\" class=\"data row9 col0\" >Tredence</td>\n",
       "      <td id=\"T_19bc3_row9_col1\" class=\"data row9 col1\" >based on 14 salaries</td>\n",
       "      <td id=\"T_19bc3_row9_col2\" class=\"data row9 col2\" > 13.9L</td>\n",
       "      <td id=\"T_19bc3_row9_col3\" class=\"data row9 col3\" >8.8L</td>\n",
       "      <td id=\"T_19bc3_row9_col4\" class=\"data row9 col4\" >17.5L</td>\n",
       "      <td id=\"T_19bc3_row9_col5\" class=\"data row9 col5\" >3 yrs experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x180dbe60760>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating empty list.\n",
    "ds_com = []\n",
    "no_sal = []\n",
    "avg_sal = []\n",
    "min_sal = []\n",
    "max_sal = []\n",
    "exp_req = []\n",
    "\n",
    "#Scraping company name\n",
    "comp_name = driver.find_elements(By.XPATH, '//div[@class=\"company-info\"]')\n",
    "for i in comp_name:\n",
    "    ds_com.append(i.text.split('\\n')[0])\n",
    "    \n",
    "#Scraping total salary record\n",
    "for i in comp_name:\n",
    "    no_sal.append(i.text.split('\\n')[-1].split('(')[-1].strip(')').strip(' '))\n",
    "\n",
    "#Scraping average salary\n",
    "av = driver.find_elements(By.XPATH, '//p[@class=\"averageCtc\"]')\n",
    "for i in av:\n",
    "    avg_sal.append(i.text.split('₹')[-1])\n",
    "    \n",
    "#Scraping minimum salary\n",
    "mini = driver.find_elements(By.XPATH, '//div[@class=\"salary-values\"]')\n",
    "for i in mini:\n",
    "    min_sal.append(i.text.split('\\n')[0].strip('₹').strip(' '))\n",
    "\n",
    "#Scraping maximum salary\n",
    "for i in mini:\n",
    "    max_sal.append(i.text.split('\\n')[1].strip('₹').strip(' '))\n",
    "    \n",
    "#Scraping experience required\n",
    "for i in comp_name:\n",
    "    exp_req.append(i.text.split('\\n')[-1].split('(')[0].strip(' '))\n",
    "    \n",
    "#creating dataframe\n",
    "DF = pd.DataFrame({'Company Name':ds_com[0:10], 'Total No Of Records':no_sal[0:10], 'Average Salary':avg_sal[0:10], 'Minimun Salary':min_sal[0:10], 'Maximum Salary':max_sal[0:10], 'Experience Required':exp_req[0:10]})\n",
    "DF = DF.style.hide_index()\n",
    "DF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
